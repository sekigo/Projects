{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93831896-683d-4946-8cae-7aff56608a65",
   "metadata": {},
   "source": [
    "## ДЗ №2. Матричные факторизации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b424ec-3791-4113-a672-25ef7cfa704d",
   "metadata": {},
   "source": [
    "#### В этой домашке вам предстоит реализовать некоторые базовые модели матричной факторизации\n",
    "\n",
    "#### Дата выдачи: 17.02.25\n",
    "\n",
    "#### Мягкий дедлайн: 02.03.25 23:59 MSK\n",
    "\n",
    "#### Жесткий дедлайн: 09.03.25 23:59 MSK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4b315-75b8-4225-953c-5510c584ff2b",
   "metadata": {},
   "source": [
    "В этом задании мы будем работать с классическим для рекоендательных систем датасетом [MovieLens 1M](https://grouplens.org/datasets/movielens/1m/). Датасет содержит рейтинги оценки для 4000 фильмов от 6000 пользователей. Более подробное описание можете найти на странице с датасетом и в README файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d3fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-2.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534ffa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 KB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/sekiprogrammer/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/sekiprogrammer/.local/lib/python3.10/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8832b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: typing\n",
      "  Building wheel for typing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26325 sha256=2f40e20c9e940f3c33cfb0a3d65bba90b3527f694cd4b631ea0f26cbf3c9c112\n",
      "  Stored in directory: /home/sekiprogrammer/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
      "Successfully built typing\n",
      "Installing collected packages: typing\n",
      "Successfully installed typing-3.7.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a85f84-4854-4249-963d-143d08bb7e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-22 14:54:00--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5917549 (5.6M) [application/zip]\n",
      "Saving to: ‘ml-1m.zip.2’\n",
      "\n",
      "ml-1m.zip.2         100%[===================>]   5.64M   766KB/s    in 7.8s    \n",
      "\n",
      "2025-02-22 14:54:08 (745 KB/s) - ‘ml-1m.zip.2’ saved [5917549/5917549]\n",
      "\n",
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat        \n",
      "  inflating: ml-1m/ratings.dat       \n",
      "  inflating: ml-1m/README            \n",
      "  inflating: ml-1m/users.dat         \n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "These files contain 1,000,209 anonymous ratings of approximately 3,900 movies \n",
      "made by 6,040 MovieLens users who joined MovieLens in 2000.\n",
      "\n",
      "USAGE LICENSE\n",
      "================================================================================\n",
      "\n",
      "Neither the University of Minnesota nor any of the researchers\n",
      "involved can guarantee the correctness of the data, its suitability\n",
      "for any particular purpose, or the validity of results based on the\n",
      "use of the data set.  The data set may be used for any research\n",
      "purposes under the following conditions:\n",
      "\n",
      "     * The user may not state or imply any endorsement from the\n",
      "       University of Minnesota or the GroupLens Research Group.\n",
      "\n",
      "     * The user must acknowledge the use of the data set in\n",
      "       publications resulting from the use of the data set\n",
      "       (see below for citation information).\n",
      "\n",
      "     * The user may not redistribute the data without separate\n",
      "       permission.\n",
      "\n",
      "     * The user may not use this information for any commercial or\n",
      "       revenue-bearing purposes without first obtaining permission\n",
      "       from a faculty member of the GroupLens Research Project at the\n",
      "       University of Minnesota.\n",
      "\n",
      "If you have any further questions or comments, please contact GroupLens\n",
      "<grouplens-info@cs.umn.edu>. \n",
      "\n",
      "CITATION\n",
      "================================================================================\n",
      "\n",
      "To acknowledge use of the dataset in publications, please cite the following\n",
      "paper:\n",
      "\n",
      "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\n",
      "and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4,\n",
      "Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872\n",
      "\n",
      "\n",
      "ACKNOWLEDGEMENTS\n",
      "================================================================================\n",
      "\n",
      "Thanks to Shyong Lam and Jon Herlocker for cleaning up and generating the data\n",
      "set.\n",
      "\n",
      "FURTHER INFORMATION ABOUT THE GROUPLENS RESEARCH PROJECT\n",
      "================================================================================\n",
      "\n",
      "The GroupLens Research Project is a research group in the Department of \n",
      "Computer Science and Engineering at the University of Minnesota. Members of \n",
      "the GroupLens Research Project are involved in many research projects related \n",
      "to the fields of information filtering, collaborative filtering, and \n",
      "recommender systems. The project is lead by professors John Riedl and Joseph \n",
      "Konstan. The project began to explore automated collaborative filtering in \n",
      "1992, but is most well known for its world wide trial of an automated \n",
      "collaborative filtering system for Usenet news in 1996. Since then the project \n",
      "has expanded its scope to research overall information filtering solutions, \n",
      "integrating in content-based methods as well as improving current collaborative \n",
      "filtering technology.\n",
      "\n",
      "Further information on the GroupLens Research project, including research \n",
      "publications, can be found at the following web site:\n",
      "        \n",
      "        http://www.grouplens.org/\n",
      "\n",
      "GroupLens Research currently operates a movie recommender based on \n",
      "collaborative filtering:\n",
      "\n",
      "        http://www.movielens.org/\n",
      "\n",
      "RATINGS FILE DESCRIPTION\n",
      "================================================================================\n",
      "\n",
      "All ratings are contained in the file \"ratings.dat\" and are in the\n",
      "following format:\n",
      "\n",
      "UserID::MovieID::Rating::Timestamp\n",
      "\n",
      "- UserIDs range between 1 and 6040 \n",
      "- MovieIDs range between 1 and 3952\n",
      "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
      "- Timestamp is represented in seconds since the epoch as returned by time(2)\n",
      "- Each user has at least 20 ratings\n",
      "\n",
      "USERS FILE DESCRIPTION\n",
      "================================================================================\n",
      "\n",
      "User information is in the file \"users.dat\" and is in the following\n",
      "format:\n",
      "\n",
      "UserID::Gender::Age::Occupation::Zip-code\n",
      "\n",
      "All demographic information is provided voluntarily by the users and is\n",
      "not checked for accuracy.  Only users who have provided some demographic\n",
      "information are included in this data set.\n",
      "\n",
      "- Gender is denoted by a \"M\" for male and \"F\" for female\n",
      "- Age is chosen from the following ranges:\n",
      "\n",
      "\t*  1:  \"Under 18\"\n",
      "\t* 18:  \"18-24\"\n",
      "\t* 25:  \"25-34\"\n",
      "\t* 35:  \"35-44\"\n",
      "\t* 45:  \"45-49\"\n",
      "\t* 50:  \"50-55\"\n",
      "\t* 56:  \"56+\"\n",
      "\n",
      "- Occupation is chosen from the following choices:\n",
      "\n",
      "\t*  0:  \"other\" or not specified\n",
      "\t*  1:  \"academic/educator\"\n",
      "\t*  2:  \"artist\"\n",
      "\t*  3:  \"clerical/admin\"\n",
      "\t*  4:  \"college/grad student\"\n",
      "\t*  5:  \"customer service\"\n",
      "\t*  6:  \"doctor/health care\"\n",
      "\t*  7:  \"executive/managerial\"\n",
      "\t*  8:  \"farmer\"\n",
      "\t*  9:  \"homemaker\"\n",
      "\t* 10:  \"K-12 student\"\n",
      "\t* 11:  \"lawyer\"\n",
      "\t* 12:  \"programmer\"\n",
      "\t* 13:  \"retired\"\n",
      "\t* 14:  \"sales/marketing\"\n",
      "\t* 15:  \"scientist\"\n",
      "\t* 16:  \"self-employed\"\n",
      "\t* 17:  \"technician/engineer\"\n",
      "\t* 18:  \"tradesman/craftsman\"\n",
      "\t* 19:  \"unemployed\"\n",
      "\t* 20:  \"writer\"\n",
      "\n",
      "MOVIES FILE DESCRIPTION\n",
      "================================================================================\n",
      "\n",
      "Movie information is in the file \"movies.dat\" and is in the following\n",
      "format:\n",
      "\n",
      "MovieID::Title::Genres\n",
      "\n",
      "- Titles are identical to titles provided by the IMDB (including\n",
      "year of release)\n",
      "- Genres are pipe-separated and are selected from the following genres:\n",
      "\n",
      "\t* Action\n",
      "\t* Adventure\n",
      "\t* Animation\n",
      "\t* Children's\n",
      "\t* Comedy\n",
      "\t* Crime\n",
      "\t* Documentary\n",
      "\t* Drama\n",
      "\t* Fantasy\n",
      "\t* Film-Noir\n",
      "\t* Horror\n",
      "\t* Musical\n",
      "\t* Mystery\n",
      "\t* Romance\n",
      "\t* Sci-Fi\n",
      "\t* Thriller\n",
      "\t* War\n",
      "\t* Western\n",
      "\n",
      "- Some MovieIDs do not correspond to a movie due to accidental duplicate\n",
      "entries and/or test entries\n",
      "- Movies are mostly entered by hand, so errors and inconsistencies may exist\n"
     ]
    }
   ],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "!unzip ml-1m.zip\n",
    "!cat ml-1m/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669137ca-f0f9-48d4-ae04-e42a4b0f8d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sekiprogrammer/.local/lib/python3.10/site-packages/numpy/_core/getlimits.py:551: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed65b0ae-ce2d-437a-9739-e7981011089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-12-31 22:12:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:35:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:32:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-12-31 22:04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>2001-01-06 23:38:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating            datetime\n",
       "0        1     1193       5 2000-12-31 22:12:40\n",
       "1        1      661       3 2000-12-31 22:35:09\n",
       "2        1      914       3 2000-12-31 22:32:48\n",
       "3        1     3408       4 2000-12-31 22:04:35\n",
       "4        1     2355       5 2001-01-06 23:38:11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ml-1m/ratings.dat\", sep='::', names=['user_id', 'item_id', 'rating', 'timestamp'], engine='python')\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.drop('timestamp', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4988d476-542e-4a32-9c45-7a16bc2a5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df['item_id'].value_counts()\n",
    "filtered_values = value_counts[value_counts > 20].index\n",
    "df = df[df['item_id'].isin(filtered_values)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e05bd3b-ab2f-4c02-9a97-a4153e9550bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((787420, 4), (207432, 4))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_end = '2000-12-01'\n",
    "df_train = df[df['datetime'] < train_end].copy()\n",
    "df_test = df[df['datetime'] >= train_end].copy()\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414b8ed7-20d6-4778-9d60-f9c8ccbfa7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106471, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_users = df_train['user_id'].unique()\n",
    "train_items = df_train['item_id'].unique()\n",
    "\n",
    "df_test = df_test[df_test['user_id'].isin(train_users)]\n",
    "df_test = df_test[df_test['item_id'].isin(train_items)]\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c724443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /home/sekiprogrammer/.local/lib/python3.10/site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f53d61-990c-4626-a470-7eefd06d2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_le = LabelEncoder()\n",
    "item_le = LabelEncoder()\n",
    "\n",
    "df_train['user_id'] = user_le.fit_transform(df_train['user_id'])\n",
    "df_train['item_id'] = item_le.fit_transform(df_train['item_id'])\n",
    "\n",
    "df_test['user_id'] = user_le.transform(df_test['user_id'])\n",
    "df_test['item_id'] = item_le.transform(df_test['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4ef3e2f-792d-4511-b4d0-74495d9cb30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3010, np.int64(3009))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['user_id'].nunique(), df_train['user_id'].max()\n",
    "df_train['item_id'].nunique(), df_train['item_id'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d356df8-9157-4752-b30d-dc80aba5e53c",
   "metadata": {},
   "source": [
    "##### Задание 1. Напишем функцию, которая превратит датафрейм в матрицу интеракций. В функции df_to_matrix реализуйте функцию, которая принимает датафрейм и возвращает np.array матрицу интеракций. В функции df_to_coo реализуйте функцию, которая принимает датафрейм и возвращает разреженную матрицу интеракций в coo_array формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cbb12158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, item_id, rating, datetime]\n",
       "Index: []"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a1d5477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100409</th>\n",
       "      <td>0</td>\n",
       "      <td>2994</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-11-30 23:49:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100411</th>\n",
       "      <td>0</td>\n",
       "      <td>929</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-11-30 23:52:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100412</th>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-11-30 23:51:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100415</th>\n",
       "      <td>0</td>\n",
       "      <td>3005</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-11-30 23:58:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100416</th>\n",
       "      <td>0</td>\n",
       "      <td>3006</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-11-30 23:57:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>5364</td>\n",
       "      <td>814</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-04-26 02:35:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>5364</td>\n",
       "      <td>817</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-04-25 23:21:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>5364</td>\n",
       "      <td>478</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-04-25 23:19:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>5364</td>\n",
       "      <td>819</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-04-26 02:20:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>5364</td>\n",
       "      <td>820</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-04-26 02:19:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>787420 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating            datetime\n",
       "100409         0     2994       3 2000-11-30 23:49:23\n",
       "100411         0      929       4 2000-11-30 23:52:33\n",
       "100412         0      567       4 2000-11-30 23:51:54\n",
       "100415         0     3005       1 2000-11-30 23:58:06\n",
       "100416         0     3006       4 2000-11-30 23:57:50\n",
       "...          ...      ...     ...                 ...\n",
       "1000204     5364      814       1 2000-04-26 02:35:41\n",
       "1000205     5364      817       5 2000-04-25 23:21:27\n",
       "1000206     5364      478       5 2000-04-25 23:19:06\n",
       "1000207     5364      819       4 2000-04-26 02:20:48\n",
       "1000208     5364      820       4 2000-04-26 02:19:29\n",
       "\n",
       "[787420 rows x 4 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f6f924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                       1\n",
      "item_id                    1193\n",
      "rating                        5\n",
      "datetime    2000-12-31 22:12:40\n",
      "Name: 0, dtype: object 0\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.iterrows():\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c81e1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df_train['user_id'].nunique()\n",
    "n_items = df_train['item_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d38a7bb7-7f2d-4957-8dac-6397af9b1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_count = 0 # для разряженной матрицы\n",
    "\n",
    "\n",
    "def df_to_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    result = np.zeros((n_users, n_items))\n",
    "    global interactions_count\n",
    "    for _, row in df.iterrows():\n",
    "        interactions_count += 1\n",
    "        result[row['user_id']][row['item_id']] += row['rating']\n",
    "        \n",
    "    print(n_users, n_items)\n",
    "    return result #shape ~ [n_users, n_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55362cbb-5e2d-455b-afd1-8507f1dd51ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5365 3010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5365, 3010)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = df_to_matrix(df_train)\n",
    "interactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ffdf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787420\n"
     ]
    }
   ],
   "source": [
    "print(interactions_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "794b646d-a560-42dd-a466-87cad2589271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_array\n",
    "\n",
    "def df_to_coo(df: pd.DataFrame) -> coo_array:\n",
    "    # global users, items, n_users, n_items\n",
    "    data = np.ones(interactions_count)\n",
    "    result = coo_array((df[\"rating\"],(df[\"user_id\"] , df[\"item_id\"])), shape = (n_users,n_items))\n",
    "    \n",
    "\n",
    "    return result # coo_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "30f9c218-0419-41b2-a112-c2214fad1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_interactions = df_to_coo(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6d6c42ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(interactions[0, 2994])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1b2fcafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495839</th>\n",
       "      <td>2369</td>\n",
       "      <td>1203</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-09-29 01:02:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating            datetime\n",
       "495839     2369     1203       5 2000-09-29 01:02:22"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[(df_train[\"user_id\"] == 2369) & (df_train[\"item_id\"] == 1203)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "96b57a7a-0551-4f75-8047-fb9904ac93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (interactions != 0).sum() == df_train.shape[0]\n",
    "assert interactions[0, 2994] == 3\n",
    "assert interactions[2369, 1203] == 5\n",
    "assert interactions[1557, 459] == 3\n",
    "assert np.allclose(coo_interactions.toarray(), interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3862a-e62a-4ebe-a70c-29102e9f99fd",
   "metadata": {},
   "source": [
    "##### Задание 2.1. Рассмотрим [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition). Возьмите готовую реализуцию алгоритма из numpy.linalg или из scipy.linalg и примените алгоритм к матрицам интеракций, полученным в первом задании. Для работы со sparse матрицей обычная реализация svd не подойдет и нужно будет воспользоваться scipy.sparse.linalg.svds. Вам нужно разложить матрицу интеракций на 3 матрицы U, S, V, а затем перемножить их и восстановить изначальную матрицу. При полном разложении исходная матрица должна восстанавливаться максимально хорошо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead134a-9aac-4027-b0b8-592de614c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_svd(interractions: Union[np.ndarray, coo_array], n_singular_values: int = -1):\n",
    "    # функция должна работать и для полной матрицы и для sparse матрицы(вам поможет isinstance).\n",
    "    # если n_singular_values = -1, то берем все сингулярные числа для полной матрицы\n",
    "    # и все кроме одного сингулярного числа для coo-матрицы(иначе scipy.sparse.linalg.svds не будет работать)\n",
    "\n",
    "\n",
    "    #your code here\n",
    "    \n",
    "    return U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fecd4-4739-4f6b-9c9c-44bde8b66a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = make_svd(interactions)\n",
    "assert np.allclose(U @ S @ V, interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5f59e-3832-4a7d-95b4-856531582bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "U1, S1, V1 = make_svd(interactions, 10)\n",
    "U, S, V = make_svd(coo_interactions, 10)\n",
    "assert np.allclose(U1 @ S1 @ V1, U @ S @ V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443daa6-e9f1-414a-8107-5a3016452929",
   "metadata": {},
   "source": [
    "##### Задание 2.2. Теперь попробуем сделать рекомендации с помощью SVD. Мы научились восстанавливать исходную матрицу с помощью разложения, теперь же мы хотим порекомендовать пользователю айтемы, которые будут для него максимально релевантны(в восстановленной матрице у них будет самый высокий скор). Для каждого пользователя нужно будет найти индексы айтемов, которые имеют максимальный скор. При этом стоит обратить внимание, что мы не хотим рекомендовать пользователю айтемы, с которыми он уже взаимодействовал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787da18f-48d0-4dfa-b18f-b7e97cc98a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_svd_recommendations(interractions: Union[np.ndarray, coo_array], n_singular_values: int = -1, top_k: int = 100):\n",
    "    # Возвращает матрицу вида n_users, top_k, то есть для каждого пользователя возвращаем индексы \n",
    "    # top_k самых релевантный айтемов среди тех с которыми он еще не взаимодействовал\n",
    "\n",
    "    #your code here\n",
    "\n",
    "\n",
    "    return recommendations #shape ~ [n_users, top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72f3dd-c8f2-4434-b84e-ef016c4773a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = make_svd_recommendations(interractions, -1, 100)\n",
    "assert recs.shape == (interractions.shape[0], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0b788-855a-4fd9-8cbb-38c0dd289ef5",
   "metadata": {},
   "source": [
    "##### Задание 2.3. Теперь давайте посмотрим как будет зависеть качетво рекомендаций, от количества сингулярных чисел, которые мы возьмем в SVD разложении. Переберите n_singular_values из списка [1, 10, 50, 200, 1000] и посмотрите как будет изменяться метрика NDCG на тестовом датасете для таких рекомендаций и как будет меняться время вычисления. Для каждого графики зависимости метрики NDCG от n_singular_values и времени работы алгоритма от n_singular_values(Время работы будет меняться только для sparse-матрицы, стоит запускать алгоритм именно для нее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f931fd-0eb0-4965-9e02-e005ed545c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(interractions: Union[np.ndarray, coo_array], top_k: int = 100):\n",
    "    #your code here\n",
    "    for n_singular_values in [1, 10, 50, 200, 1000]:\n",
    "        #your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22fe02-b1c2-4fb5-a7b1-0525356fe9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8612d96-c694-42d6-9525-58808f642577",
   "metadata": {},
   "source": [
    "##### Задание 3.1. Перейдем к [ALS](http://yifanhu.net/PUB/cf.pdf). Возьмем реализацию iALS из библиотеки [implicit](https://benfred.github.io/implicit/api/models/cpu/als.html). Обучите ALS на нашем датасете, сделайте top_k рекомендации для юзеров из тестового датасета, и сравните метрики ALS с метриками, которые получились в SVD. Попробуйте перебрать гиперпараметры и найдите оптимальное число факторов, коэффициент alpha и коэффициент регуляризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb16e9-49d1-4d7e-a384-ccbdad4f1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_als_recommendations(\n",
    "    interractions: Union[np.ndarray, coo_array], \n",
    "    top_k: int = 100, \n",
    "    n_factors: int = 100,\n",
    "    alpha: float = 1.0,\n",
    "    regularization: float = 0.01,\n",
    "):\n",
    "    #your code here\n",
    "\n",
    "\n",
    "    return recommendations #shape ~ [n_users, top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5573d01-8512-450f-9260-03928ee9b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = make_als_recommendations(interractions)\n",
    "assert recs.shape == (interractions.shape[0], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3b1cc-760b-4d6a-ab8a-aca57b7b6dfe",
   "metadata": {},
   "source": [
    "##### Задание 3.2. Сделайте объяснение рекомендаций для нескольких юзеров(als.explain). Воспользуйтесь файлом movies.dat чтобы перейти от индексов фильмов к их названием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9a956-d2f5-4560-8d0c-c72164ebdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7ea0c-8288-45f5-83d4-9761a8952669",
   "metadata": {},
   "source": [
    "##### Задание 4. До этого мы работали с рейтингами, но как обсуждалось на лекции, implicit ALS отлично работает и с implicit фидбэком. Давайте попробуем преобразовать наш датасет(трейн и тест) следующим образом\n",
    "\n",
    "1. Бинаризуем все рейтинги(заменим любую интеракцию пользователя на 1)\n",
    "2. Заменим на 1 только рейтинги 4 и 5, а рейтинг ниже 4 заменим на 0\n",
    "3. Заменим на 1 только рейтинги 4 и 5, а рейтинг ниже 4 заменим на -1\n",
    "4. Заменим на 1 только рейтинги 4 и 5, а рейтинг ниже 4 заменим на -1 и добавим сглаживание по времени. То есть чем дальше была интеракция от максимальной даты трейна, тем с меньшим весом мы будем ее учитывать(например можно интеракции за последний месяц брать в исходном виде, и с каждым месяцем в прошлое умножать их на какой-нибудь коэффициент меньший 1). Таким образом более старые интеракции пользователя будут вносить меньший вклад в его интересы\n",
    "5. Придумайте свой вариант(опционально)\n",
    "\n",
    "Для каждой полученной матрицы обучите iALS и SVD и сравните их результаты между собой(преобразовывать нужно только обучающую выборку, тестовую оставляем неизменной)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839fd09-eca4-43e0-a209-2da63775f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a03b1-5072-4bc4-ba46-1fc3c989b888",
   "metadata": {},
   "source": [
    "##### Задание 5. iALS на numpy/torch. Давайте реализуем алгоритм iALS на нумпае или торче. Требуется реализовать алгорит, описанный в 4 части [статьи](http://yifanhu.net/PUB/cf.pdf). Обратите внимания на все оптимизации, которые они описывают в статье, чтобы сократить лишние вычисления. Hint: метрики у вашего алгоритма должны быть сравнимы с метриками ALS из библиотеки implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be5f0d-1454-4f15-9dd1-17d1f1f99583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iALS:\n",
    "    def __init__(self, n_factors: int = 100, alpha: float = 1.0, reg_coef = 0.01):\n",
    "        #your code here\n",
    "\n",
    "    def fit(self, interractions: np.ndarray, n_iterations: int 10):\n",
    "        #your code here\n",
    "\n",
    "    def predict(self, top_k: int = 100):\n",
    "        # возвращает top-k айтемов для каждого юзера(айтемы с которыми юзер взаимодействовал не должны попасть в рекомендации)\n",
    "        #your code here\n",
    "\n",
    "        return predicts # shape ~ [n_users, top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa65d7-11dc-48d9-a270-8d0a6d96f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
